{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aniket/miniconda3/envs/parrot/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llm_chain import LitGPTConversationChain, LitGPTLLM\n",
    "from llm_inference import prepare_weights\n",
    "from rich import print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, I am Aniket!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Hi, I am Aniket!'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'history'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'response'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Hi, I am a helpful chatbot!'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\u001b[32m'input'\u001b[0m: \u001b[32m'Hi, I am Aniket!'\u001b[0m, \u001b[32m'history'\u001b[0m: \u001b[32m''\u001b[0m, \u001b[32m'response'\u001b[0m: \u001b[32m'Hi, I am a helpful chatbot!'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bot = LitGPTConversationChain.from_llm(llm=\"dummy\", verbose=True)\n",
    "print(bot(\"Hi, I am Aniket!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = str(prepare_weights(\"EleutherAI/pythia-70m\"))\n",
    "# llm = LitGPTLLM(checkpoint_dir=path, accelerator=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights already exists at checkpoints/lmsys/longchat-13b-16k\n"
     ]
    }
   ],
   "source": [
    "path = str(prepare_weights(\"lmsys/longchat-13b-16k\"))\n",
    "llm = LitGPTLLM(checkpoint_dir=path, quantize=\"bnb.nf4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'input'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'history'</span><span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">output_parser</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">detailed, and polite answers to the user's questions.\\nContext: {history}\\nUSER: {input} ASSISTANT:\\n\"</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">template_format</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'f-string'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">validate_template</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'input'\u001b[0m, \u001b[32m'history'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[33moutput_parser\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mtemplate\u001b[0m=\u001b[32m\"A\u001b[0m\u001b[32m chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, \u001b[0m\n",
       "\u001b[32mdetailed, and polite answers to the user's questions.\\nContext: \u001b[0m\u001b[32m{\u001b[0m\u001b[32mhistory\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\nUSER: \u001b[0m\u001b[32m{\u001b[0m\u001b[32minput\u001b[0m\u001b[32m}\u001b[0m\u001b[32m ASSISTANT:\\n\"\u001b[0m,\n",
       "    \u001b[33mtemplate_format\u001b[0m=\u001b[32m'f-string'\u001b[0m,\n",
       "    \u001b[33mvalidate_template\u001b[0m=\u001b[3;92mTrue\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llm_chain.templates import longchat_prompt_template\n",
    "\n",
    "print(longchat_prompt_template)\n",
    "\n",
    "bot = LitGPTConversationChain.from_llm(\n",
    "    llm=llm, prompt=longchat_prompt_template, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mA chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
      "Context: \n",
      "USER: Hi, I am Adam ASSISTANT:\n",
      "\u001b[0m\n",
      "Loading model for first time...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Loading model 'checkpoints/lmsys/longchat-13b-16k/lit_model.pth' with {'org': 'lmsys', 'name': 'longchat-13b-16k', 'block_size': 2048, 'vocab_size': 32000, 'padding_multiple': 64, 'padded_vocab_size': 32000, 'n_layer': 40, 'n_head': 40, 'n_embd': 5120, 'rotary_percentage': 1.0, 'parallel_residual': False, 'bias': False, 'n_query_groups': 40, 'shared_attention_norm': False, '_norm_class': 'RMSNorm', 'norm_eps': 1e-06, '_mlp_class': 'LLaMAMLP', 'intermediate_size': 13824, 'condense_ratio': 8}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin /home/aniket/miniconda3/envs/parrot/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda121.so\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time to instantiate model: 14.66 seconds.\n",
      "Time to load the model weights: 21.27 seconds.\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time for inference: 4.22 sec total, 6.39 tokens/sec\n",
      "Memory used: 8.40 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Hello, I am Adam. How can I help you today? If you have any questions or need assistance, feel free to ask.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Hello, I am Adam. How can I help you today? If you have any questions or need assistance, feel free to ask.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(bot.send(\"Hi, I am Adam\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mA chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
      "Context: User: Hi, I am Adam\n",
      "Assistant: Hello, I am Adam. How can I help you today? If you have any questions or need assistance, feel free to ask.\n",
      "USER: What is the timezone of London? ASSISTANT:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time for inference: 2.56 sec total, 17.55 tokens/sec\n",
      "Memory used: 8.43 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The timezone of London is Greenwich Mean Time <span style=\"font-weight: bold\">(</span>GMT<span style=\"font-weight: bold\">)</span>. However, it's important to note that London does not follow \n",
       "Daylight Saving Time <span style=\"font-weight: bold\">(</span>DST<span style=\"font-weight: bold\">)</span>, so it remains in GMT year-round.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "The timezone of London is Greenwich Mean Time \u001b[1m(\u001b[0mGMT\u001b[1m)\u001b[0m. However, it's important to note that London does not follow \n",
       "Daylight Saving Time \u001b[1m(\u001b[0mDST\u001b[1m)\u001b[0m, so it remains in GMT year-round.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(bot.send(\"What is the timezone of London?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mA chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
      "Context: User: Hi, I am Adam\n",
      "Assistant: Hello, I am Adam. How can I help you today? If you have any questions or need assistance, feel free to ask.\n",
      "User: What is the timezone of London?\n",
      "Assistant: The timezone of London is Greenwich Mean Time (GMT). However, it's important to note that London does not follow Daylight Saving Time (DST), so it remains in GMT year-round.\n",
      "USER: my monitor screen size is 32 inch. ASSISTANT:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time for inference: 2.41 sec total, 25.68 tokens/sec\n",
      "Memory used: 8.48 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThe screen size of your monitor is 32 inches. This is a common screen size for computer monitors and is often used for gaming, video editing, and other demanding tasks that require a large display. If you have any questions or need assistance with your monitor, feel free to ask.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.send(\"my monitor screen size is 32 inch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mA chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
      "Context: User: Hi, I am Adam\n",
      "Assistant: Hello, I am Adam. How can I help you today? If you have any questions or need assistance, feel free to ask.\n",
      "User: What is the timezone of London?\n",
      "Assistant: The timezone of London is Greenwich Mean Time (GMT). However, it's important to note that London does not follow Daylight Saving Time (DST), so it remains in GMT year-round.\n",
      "User: my monitor screen size is 32 inch.\n",
      "Assistant: \n",
      "The screen size of your monitor is 32 inches. This is a common screen size for computer monitors and is often used for gaming, video editing, and other demanding tasks that require a large display. If you have any questions or need assistance with your monitor, feel free to ask.\n",
      "USER: What did I tell you about my monitor screen size? ASSISTANT:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time for inference: 2.38 sec total, 26.42 tokens/sec\n",
      "Memory used: 8.55 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'You told me that your monitor screen size is 32 inches. This is a common screen size for computer monitors and is often used for gaming, video editing, and other demanding tasks that require a large display. If you have any questions or need assistance with your monitor, feel free to ask.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.send(\"What did I tell you about my monitor screen size?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mA chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
      "Context: \n",
      "USER: What did I tell you about my monitor screen size? ASSISTANT:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time for inference: 1.06 sec total, 23.65 tokens/sec\n",
      "Memory used: 8.55 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'You did not provide any information about your monitor screen size. Please let me know if you have any other questions or concerns.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.clear()  # clears the chatbot memory\n",
    "bot.send(\"What did I tell you about my monitor screen size?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Managing the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here it is by default set to \"AI\"\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    prompt=longchat_prompt_template,\n",
    "    verbose=False,\n",
    "    memory=ConversationBufferMemory(ai_prefix=\"Assistant\", human_prefix=\"User\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time for inference: 3.82 sec total, 26.19 tokens/sec\n",
      "Memory used: 8.61 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">PyTorch Lightning is an open-source library developed by the Lightning AI team. It is designed to provide efficient\n",
       "and scalable solutions for various natural language processing <span style=\"font-weight: bold\">(</span>NLP<span style=\"font-weight: bold\">)</span> tasks, such as text classification, sentiment \n",
       "analysis, and machine translation. The library is built on top of the PyTorch framework, which is a popular and \n",
       "powerful deep learning library.\n",
       "\n",
       "PyTorch Lightning offers a collection of pre-trained models and tools that can be easily integrated into\n",
       "</pre>\n"
      ],
      "text/plain": [
       "PyTorch Lightning is an open-source library developed by the Lightning AI team. It is designed to provide efficient\n",
       "and scalable solutions for various natural language processing \u001b[1m(\u001b[0mNLP\u001b[1m)\u001b[0m tasks, such as text classification, sentiment \n",
       "analysis, and machine translation. The library is built on top of the PyTorch framework, which is a popular and \n",
       "powerful deep learning library.\n",
       "\n",
       "PyTorch Lightning offers a collection of pre-trained models and tools that can be easily integrated into\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = conversation(\"PyTorch Lightning is an open-source library developed by Lightning AI team.\")[\"response\"]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time for inference: 1.78 sec total, 24.78 tokens/sec\n",
      "Memory used: 8.61 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The PyTorch Lightning library was developed by the Lightning AI team, which consists of experienced researchers and\n",
       "engineers working on various NLP tasks, including text classification, sentiment analysis, and machine translation.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "The PyTorch Lightning library was developed by the Lightning AI team, which consists of experienced researchers and\n",
       "engineers working on various NLP tasks, including text classification, sentiment analysis, and machine translation.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = conversation(\"who developed PyTorch Lightning? just give me the name of the team or person and nothing else.\")[\"response\"]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "am",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
