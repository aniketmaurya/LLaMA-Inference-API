{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Generation at high level\n",
    "\n",
    "```python\n",
    "query = \"Capital of\"\n",
    "\n",
    "output = \"\"\n",
    "for i in range(MAX_GENERATED_TOKENS):\n",
    "    output = output + LLM(output)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. output = Capital of\n",
    "1. output = Capital of <mark>France</mark>\n",
    "1. output = Capital of France <mark>is</mark>\n",
    "1. output = Capital of France is <mark>Paris</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "query = A chat between a curious user and an artificial intelligence assistant.\n",
    "        The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
    "        USER: My name is Aniket\n",
    "        ASSISTANT:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "longchat_template = \"\"\"A chat between a curious user and an artificial intelligence assistant.\n",
    "The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
    "USER: {input}\n",
    "ASSISTANT:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A chat between a curious user and an artificial intelligence assistant.\n",
      "The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
      "USER: My name is Aniket\n",
      "ASSISTANT:\n"
     ]
    }
   ],
   "source": [
    "print(longchat_template.format(input=\"My name is Aniket\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A chat between a curious user and an artificial intelligence assistant.\n",
      "The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
      "USER: What is the capital of France?\n",
      "ASSISTANT:\n"
     ]
    }
   ],
   "source": [
    "print(longchat_template.format(input=\"What is the capital of France?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_inference import LLMInference, prepare_weights\n",
    "from rich import print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = str(prepare_weights(\"meta-llama/Llama-2-7b-chat-hf\"))\n",
    "# model = LLMInference(checkpoint_dir=path, quantize=\"bnb.nf4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights already exists at checkpoints/lmsys/longchat-7b-16k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model 'checkpoints/lmsys/longchat-7b-16k/lit_model.pth' with {'org': 'lmsys', 'name': 'longchat-7b-16k', 'block_size': 16384, 'vocab_size': 32000, 'padding_multiple': 64, 'padded_vocab_size': 32000, 'n_layer': 32, 'n_head': 32, 'n_embd': 4096, 'rotary_percentage': 1.0, 'parallel_residual': False, 'bias': False, 'n_query_groups': 32, 'shared_attention_norm': False, '_norm_class': 'RMSNorm', 'norm_eps': 1e-06, '_mlp_class': 'LLaMAMLP', 'intermediate_size': 11008, 'condense_ratio': 8}\n",
      "Time to instantiate model: 4.83 seconds.\n",
      "Time to load the model weights: 10.19 seconds.\n",
      "/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/lightning/fabric/fabric.py:943: The model passed to `Fabric.setup()` has 66 parameters on different devices (for example 'transformer.wte.weight' on cuda:0 and 'lm_head.weight' on cpu). Since `move_to_device=True`, all parameters will be moved to the new device. If this is not desired, set `Fabric.setup(..., move_to_device=False)`.\n"
     ]
    }
   ],
   "source": [
    "path = str(prepare_weights(\"lmsys/longchat-7b-16k\"))\n",
    "model = LLMInference(checkpoint_dir=path, quantize=\"bnb.nf4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "longchat_template = \"\"\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
    "USER: {input}\n",
    "ASSISTANT:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time for inference: 1.13 sec total, 6.22 tokens/sec\n",
      "Memory used: 13.40 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The capital of France is Paris.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "The capital of France is Paris.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = longchat_template.format(input=\"What is the capital of France?\")\n",
    "output = model.chat(query)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time for inference: 0.70 sec total, 17.13 tokens/sec\n",
      "Memory used: 13.40 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Hello Aniket! How can I assist you today?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Hello Aniket! How can I assist you today?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = model.chat(longchat_template.format(input=\"My name is Aniket\"))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time for inference: 10.81 sec total, 22.66 tokens/sec\n",
      "Memory used: 13.40 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Lightning strikes with power and might,\n",
       "A force that shatters through the night,\n",
       "A bolt of energy that doth ignite,\n",
       "A flash of light that doth enthrall.\n",
       "\n",
       "In the heart of the storm, it doth reside,\n",
       "A tempest of energy, it doth ride,\n",
       "A force that doth dazzle and amaze,\n",
       "A wonder of nature, it doth raise.\n",
       "\n",
       "With speed that's faster than the eye can see,\n",
       "It doth traverse the sky with great ease,\n",
       "A bolt of light that doth illuminate,\n",
       "A sight that's truly awe-inspiring.\n",
       "\n",
       "It's power is a force to be reckoned,\n",
       "A force that's truly untamed,\n",
       "A force that doth doth amaze,\n",
       "A force that's truly unrefined.\n",
       "\n",
       "So here's to the lightning, a force divine,\n",
       "A force that's truly one of a kind,\n",
       "A force that doth dazzle and astound,\n",
       "A force that's truly one of a kind.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Lightning strikes with power and might,\n",
       "A force that shatters through the night,\n",
       "A bolt of energy that doth ignite,\n",
       "A flash of light that doth enthrall.\n",
       "\n",
       "In the heart of the storm, it doth reside,\n",
       "A tempest of energy, it doth ride,\n",
       "A force that doth dazzle and amaze,\n",
       "A wonder of nature, it doth raise.\n",
       "\n",
       "With speed that's faster than the eye can see,\n",
       "It doth traverse the sky with great ease,\n",
       "A bolt of light that doth illuminate,\n",
       "A sight that's truly awe-inspiring.\n",
       "\n",
       "It's power is a force to be reckoned,\n",
       "A force that's truly untamed,\n",
       "A force that doth doth amaze,\n",
       "A force that's truly unrefined.\n",
       "\n",
       "So here's to the lightning, a force divine,\n",
       "A force that's truly one of a kind,\n",
       "A force that doth dazzle and astound,\n",
       "A force that's truly one of a kind.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = model.chat(longchat_template.format(input=\"Write a poem on Lightning AI\"))\n",
    "print(output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time for inference: 0.70 sec total, 17.26 tokens/sec\n",
      "Memory used: 13.40 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Hello Aniket! How can I assist you today?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Hello Aniket! How can I assist you today?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time for inference: 1.52 sec total, 21.09 tokens/sec\n",
      "Memory used: 13.40 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">I'm sorry, but I don't have access to your personal information. Can you please provide me with your name so I can \n",
       "assist you better?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "I'm sorry, but I don't have access to your personal information. Can you please provide me with your name so I can \n",
       "assist you better?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = model.chat(longchat_template.format(input=\"My name is Aniket?\"))\n",
    "print(output)\n",
    "\n",
    "\n",
    "output = model.chat(longchat_template.format(input=\"What is my name?\"))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "longchat_template = \"\"\"A chat between a curious user and an artificial intelligence assistant.\n",
    "The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
    "Context:\n",
    "User: My name is Aniket\n",
    "Assistant: Hi, Aniket how are you?\n",
    "\n",
    "USER: {input}\n",
    "ASSISTANT:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time for inference: 0.46 sec total, 15.37 tokens/sec\n",
      "Memory used: 13.40 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Your name is Aniket.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Your name is Aniket.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "longchat_template = \"\"\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
    "Context:\n",
    "USER: My name is Aniket!\n",
    "ASSISTANT: How can I help you Aniket?\n",
    "USER: {input}\n",
    "ASSISTANT:\"\"\"\n",
    "\n",
    "output = model.chat(longchat_template.format(input=\"What is my name?\"))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time for inference: 0.45 sec total, 15.39 tokens/sec\n",
      "Memory used: 13.40 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Your name is Aniket.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Your name is Aniket.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "longchat_template = \"\"\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
    "Context:\n",
    "{history}\n",
    "USER: {input}\n",
    "ASSISTANT:\"\"\"\n",
    "\n",
    "history =\"USER: Hi, I am Aniket!\\nAssistant: How can I help you Aniket?\"\n",
    "\n",
    "query = longchat_template.format(input=\"What is my name?\", history=history)\n",
    "output = model.chat(query)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[PromptTemplate doc](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "longchat_template = \"\"\"A chat between a curious user and an artificial intelligence assistant.\n",
    "The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
    "Context:\n",
    "{history}\n",
    "USER: {input}\n",
    "ASSISTANT:\"\"\"\n",
    "\n",
    "longchat_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"input\", \"history\"], template=longchat_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">A chat between a curious user and an artificial intelligence assistant.\n",
       "The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
       "Context:\n",
       "USER: Hi, I am Aniket!\n",
       "Assistant: How can I help you Aniket?\n",
       "USER: What is my name?\n",
       "ASSISTANT:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "A chat between a curious user and an artificial intelligence assistant.\n",
       "The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
       "Context:\n",
       "USER: Hi, I am Aniket!\n",
       "Assistant: How can I help you Aniket?\n",
       "USER: What is my name?\n",
       "ASSISTANT:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(longchat_prompt_template.format(\n",
    "    input = \"What is my name?\",\n",
    "    history =\"USER: Hi, I am Aniket!\\nAssistant: How can I help you Aniket?\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "from llm_chain import LitGPTLLM\n",
    "\n",
    "\n",
    "llm = LitGPTLLM(model=model)\n",
    "\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    prompt=longchat_prompt_template,\n",
    "    verbose=False,\n",
    "    memory=ConversationBufferWindowMemory(ai_prefix=\"Assistant\", human_prefix=\"User\", k=2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time for inference: 0.68 sec total, 17.68 tokens/sec\n",
      "Memory used: 13.40 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello Aniket! How can I assist you today?'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation(\"hi, I am Aniket\")[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time for inference: 0.43 sec total, 16.36 tokens/sec\n",
      "Memory used: 13.40 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Aniket.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation(\"What is my name?\")[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time for inference: 0.73 sec total, 19.09 tokens/sec\n",
      "Memory used: 13.40 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'London is in the GMT (Greenwich Mean Time) timezone.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation(\"What is the timezone of London?\")[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatMessageHistory</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">messages</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'hi, I am Aniket'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>, <span style=\"color: #808000; text-decoration-color: #808000\">example</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Hello Aniket! How can I assist you today?'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>, <span style=\"color: #808000; text-decoration-color: #808000\">example</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'What is my name?'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>, <span style=\"color: #808000; text-decoration-color: #808000\">example</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Your name is Aniket.'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>, <span style=\"color: #808000; text-decoration-color: #808000\">example</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'What is the timezone of London?'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>, <span style=\"color: #808000; text-decoration-color: #808000\">example</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'London is in the GMT (Greenwich Mean Time) timezone.'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">example</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatMessageHistory\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mmessages\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mHumanMessage\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcontent\u001b[0m=\u001b[32m'hi, I am Aniket'\u001b[0m, \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m, \u001b[33mexample\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcontent\u001b[0m=\u001b[32m'Hello Aniket! How can I assist you today?'\u001b[0m, \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m, \u001b[33mexample\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mHumanMessage\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcontent\u001b[0m=\u001b[32m'What is my name?'\u001b[0m, \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m, \u001b[33mexample\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcontent\u001b[0m=\u001b[32m'Your name is Aniket.'\u001b[0m, \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m, \u001b[33mexample\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mHumanMessage\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcontent\u001b[0m=\u001b[32m'What is the timezone of London?'\u001b[0m, \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m, \u001b[33mexample\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mcontent\u001b[0m=\u001b[32m'London is in the GMT \u001b[0m\u001b[32m(\u001b[0m\u001b[32mGreenwich Mean Time\u001b[0m\u001b[32m)\u001b[0m\u001b[32m timezone.'\u001b[0m,\n",
       "            \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mexample\u001b[0m=\u001b[3;91mFalse\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(conversation.memory.chat_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    prompt=longchat_prompt_template,\n",
    "    verbose=False,\n",
    "    memory=ConversationBufferMemory(ai_prefix=\"Assistant\", human_prefix=\"User\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = conversation(\n",
    "    \"PyTorch Lightning is an open-source library developed by Lightning AI team.\"\n",
    ")[\"response\"]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = conversation(\n",
    "    \"who developed PyTorch Lightning? just give me the name of the team or person and nothing else.\"\n",
    ")[\"response\"]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://twitter.com/yanndubs/status/1681644889145237504?s=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
