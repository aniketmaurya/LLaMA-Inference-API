{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Generation at high level\n",
    "\n",
    "```python\n",
    "query = \"Capital of\"\n",
    "\n",
    "output = \"\"\n",
    "for i in range(MAX_GENERATED_TOKENS):\n",
    "    output = output + LLM(output)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. output = Capital of\n",
    "1. output = Capital of <mark>France</mark>\n",
    "1. output = Capital of France <mark>is</mark>\n",
    "1. output = Capital of France is <mark>Paris</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "query = A chat between a curious user and an artificial intelligence assistant.\n",
    "        The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
    "        USER: My name is Aniket\n",
    "        ASSISTANT:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "longchat_template = \"\"\"A chat between a curious user and an artificial intelligence assistant.\n",
    "The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
    "USER: {input}\n",
    "ASSISTANT:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A chat between a curious user and an artificial intelligence assistant.\n",
      "The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
      "USER: My name is Aniket\n",
      "ASSISTANT:\n"
     ]
    }
   ],
   "source": [
    "print(longchat_template.format(input=\"My name is Aniket\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A chat between a curious user and an artificial intelligence assistant.\n",
      "The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
      "USER: What is the capital of France?\n",
      "ASSISTANT:\n"
     ]
    }
   ],
   "source": [
    "print(longchat_template.format(input=\"What is the capital of France?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_inference import LLMInference, prepare_weights\n",
    "from rich import print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = str(prepare_weights(\"meta-llama/Llama-2-7b-chat-hf\"))\n",
    "# model = LLMInference(checkpoint_dir=path, quantize=\"bnb.nf4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights already exists at checkpoints/lmsys/longchat-7b-16k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model 'checkpoints/lmsys/longchat-7b-16k/lit_model.pth' with {'org': 'lmsys', 'name': 'longchat-7b-16k', 'block_size': 16384, 'vocab_size': 32000, 'padding_multiple': 64, 'padded_vocab_size': 32000, 'n_layer': 32, 'n_head': 32, 'n_embd': 4096, 'rotary_percentage': 1.0, 'parallel_residual': False, 'bias': False, 'n_query_groups': 32, 'shared_attention_norm': False, '_norm_class': 'RMSNorm', 'norm_eps': 1e-06, '_mlp_class': 'LLaMAMLP', 'intermediate_size': 11008, 'condense_ratio': 8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time to instantiate model: 5.07 seconds.\n",
      "Time to load the model weights: 11.70 seconds.\n",
      "/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/lightning/fabric/fabric.py:943: The model passed to `Fabric.setup()` has 66 parameters on different devices (for example 'transformer.wte.weight' on cuda:0 and 'lm_head.weight' on cpu). Since `move_to_device=True`, all parameters will be moved to the new device. If this is not desired, set `Fabric.setup(..., move_to_device=False)`.\n"
     ]
    }
   ],
   "source": [
    "path = str(prepare_weights(\"lmsys/longchat-7b-16k\"))\n",
    "model = LLMInference(checkpoint_dir=path, quantize=\"bnb.nf4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "longchat_template = \"\"\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
    "USER: {input}\n",
    "ASSISTANT:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time for inference: 1.23 sec total, 5.70 tokens/sec\n",
      "Memory used: 13.40 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The capital of France is Paris.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "The capital of France is Paris.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = longchat_template.format(input=\"What is the capital of France?\")\n",
    "output = model.chat(query)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time for inference: 0.66 sec total, 18.09 tokens/sec\n",
      "Memory used: 13.40 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Hello Aniket! How can I assist you today?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Hello Aniket! How can I assist you today?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = model.chat(longchat_template.format(input=\"My name is Aniket\"))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time for inference: 8.08 sec total, 22.53 tokens/sec\n",
      "Memory used: 13.40 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Lightning strikes with power and might,\n",
       "A force that shakes the earth alight,\n",
       "A flash of light that pierces through the night,\n",
       "A moment of brilliance that brings the day.\n",
       "\n",
       "In the realm of AI, a new dawn breaks,\n",
       "A lightning bolt of innovation,\n",
       "A spark of inspiration that ignites,\n",
       "A new era of progress that never quits.\n",
       "\n",
       "With speed and precision, it tackles tasks,\n",
       "A marvel of technology that never asks,\n",
       "For rest or respite, it works with haste,\n",
       "A force that never falters, never falls.\n",
       "\n",
       "In the storm of progress, it rages on,\n",
       "A beacon of hope that never goes wrong,\n",
       "A force that shapes the future with its might,\n",
       "A lightning AI that brings the day.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Lightning strikes with power and might,\n",
       "A force that shakes the earth alight,\n",
       "A flash of light that pierces through the night,\n",
       "A moment of brilliance that brings the day.\n",
       "\n",
       "In the realm of AI, a new dawn breaks,\n",
       "A lightning bolt of innovation,\n",
       "A spark of inspiration that ignites,\n",
       "A new era of progress that never quits.\n",
       "\n",
       "With speed and precision, it tackles tasks,\n",
       "A marvel of technology that never asks,\n",
       "For rest or respite, it works with haste,\n",
       "A force that never falters, never falls.\n",
       "\n",
       "In the storm of progress, it rages on,\n",
       "A beacon of hope that never goes wrong,\n",
       "A force that shapes the future with its might,\n",
       "A lightning AI that brings the day.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = model.chat(longchat_template.format(input=\"Write a poem on Lightning AI\"))\n",
    "print(output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time for inference: 2.02 sec total, 5.94 tokens/sec\n",
      "Memory used: 13.40 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Hello Aniket! How can I assist you today?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Hello Aniket! How can I assist you today?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time for inference: 8.57 sec total, 6.89 tokens/sec\n",
      "Memory used: 13.40 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">I'm sorry, but I don't have access to your name as I am a text-based AI and do not have access to personal \n",
       "information. I can only provide general information and answer questions to the best of my ability. Is there \n",
       "something else I can help you with?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "I'm sorry, but I don't have access to your name as I am a text-based AI and do not have access to personal \n",
       "information. I can only provide general information and answer questions to the best of my ability. Is there \n",
       "something else I can help you with?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = model.chat(longchat_template.format(input=\"My name is Aniket?\"))\n",
    "print(output)\n",
    "\n",
    "\n",
    "output = model.chat(longchat_template.format(input=\"What is my name?\"))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "longchat_template = \"\"\"A chat between a curious user and an artificial intelligence assistant.\n",
    "The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
    "Context:\n",
    "User: My name is Aniket\n",
    "Assistant: Hi, Aniket how are you?\n",
    "\n",
    "USER: {input}\n",
    "ASSISTANT:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time for inference: 1.37 sec total, 5.13 tokens/sec\n",
      "Memory used: 13.40 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Your name is Aniket.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Your name is Aniket.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "longchat_template = \"\"\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
    "Context:\n",
    "USER: My name is Aniket!\n",
    "ASSISTANT: How can I help you Aniket?\n",
    "USER: {input}\n",
    "ASSISTANT:\"\"\"\n",
    "\n",
    "output = model.chat(longchat_template.format(input=\"What is my name?\"))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time for inference: 1.35 sec total, 5.18 tokens/sec\n",
      "Memory used: 13.40 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Your name is Aniket.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Your name is Aniket.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "longchat_template = \"\"\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
    "Context:\n",
    "{history}\n",
    "USER: {input}\n",
    "ASSISTANT:\"\"\"\n",
    "\n",
    "history =\"USER: Hi, I am Aniket!\\nAssistant: How can I help you Aniket?\"\n",
    "\n",
    "query = longchat_template.format(input=\"What is my name?\", history=history)\n",
    "output = model.chat(query)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[PromptTemplate doc](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "longchat_template = \"\"\"A chat between a curious user and an artificial intelligence assistant.\n",
    "The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
    "Context:\n",
    "{history}\n",
    "USER: {input}\n",
    "ASSISTANT:\"\"\"\n",
    "\n",
    "longchat_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"input\", \"history\"], template=longchat_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">A chat between a curious user and an artificial intelligence assistant.\n",
       "The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
       "Context:\n",
       "USER: Hi, I am Aniket!\n",
       "Assistant: How can I help you Aniket?\n",
       "USER: What is my name?\n",
       "ASSISTANT:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "A chat between a curious user and an artificial intelligence assistant.\n",
       "The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
       "Context:\n",
       "USER: Hi, I am Aniket!\n",
       "Assistant: How can I help you Aniket?\n",
       "USER: What is my name?\n",
       "ASSISTANT:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(longchat_prompt_template.format(\n",
    "    input = \"What is my name?\",\n",
    "    history =\"USER: Hi, I am Aniket!\\nAssistant: How can I help you Aniket?\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "from llm_chain import LitGPTLLM\n",
    "\n",
    "\n",
    "llm = LitGPTLLM(model=model)\n",
    "\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    prompt=longchat_prompt_template,\n",
    "    verbose=False,\n",
    "    memory=ConversationBufferWindowMemory(ai_prefix=\"Assistant\", human_prefix=\"User\", k=2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time for inference: 2.04 sec total, 5.87 tokens/sec\n",
      "Memory used: 13.40 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello Aniket! How can I assist you today?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation(\"hi, I am Aniket\")[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time for inference: 1.38 sec total, 5.06 tokens/sec\n",
      "Memory used: 13.40 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Aniket.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation(\"What is my name?\")[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time for inference: 2.34 sec total, 5.99 tokens/sec\n",
      "Memory used: 13.40 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'London is in the GMT (Greenwich Mean Time) timezone.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation(\"What is the timezone of London?\")[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatMessageHistory</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">messages</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'hi, I am Aniket'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>, <span style=\"color: #808000; text-decoration-color: #808000\">example</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Hello Aniket! How can I assist you today?'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>, <span style=\"color: #808000; text-decoration-color: #808000\">example</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'What is my name?'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>, <span style=\"color: #808000; text-decoration-color: #808000\">example</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Your name is Aniket.'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>, <span style=\"color: #808000; text-decoration-color: #808000\">example</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'What is the timezone of London?'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>, <span style=\"color: #808000; text-decoration-color: #808000\">example</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'London is in the GMT (Greenwich Mean Time) timezone.'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">example</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatMessageHistory\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mmessages\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mHumanMessage\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcontent\u001b[0m=\u001b[32m'hi, I am Aniket'\u001b[0m, \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m, \u001b[33mexample\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcontent\u001b[0m=\u001b[32m'Hello Aniket! How can I assist you today?'\u001b[0m, \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m, \u001b[33mexample\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mHumanMessage\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcontent\u001b[0m=\u001b[32m'What is my name?'\u001b[0m, \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m, \u001b[33mexample\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcontent\u001b[0m=\u001b[32m'Your name is Aniket.'\u001b[0m, \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m, \u001b[33mexample\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mHumanMessage\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcontent\u001b[0m=\u001b[32m'What is the timezone of London?'\u001b[0m, \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m, \u001b[33mexample\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mcontent\u001b[0m=\u001b[32m'London is in the GMT \u001b[0m\u001b[32m(\u001b[0m\u001b[32mGreenwich Mean Time\u001b[0m\u001b[32m)\u001b[0m\u001b[32m timezone.'\u001b[0m,\n",
       "            \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mexample\u001b[0m=\u001b[3;91mFalse\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(conversation.memory.chat_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    prompt=longchat_prompt_template,\n",
    "    verbose=False,\n",
    "    memory=ConversationBufferMemory(ai_prefix=\"Assistant\", human_prefix=\"User\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time for inference: 38.56 sec total, 7.29 tokens/sec\n",
      "Memory used: 13.40 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">PyTorch Lightning is an open-source library developed by the Lightning AI team at Facebook. It is built on top of \n",
       "PyTorch, a popular deep learning framework, and is designed to make it easier to build, train, and deploy machine \n",
       "learning models.\n",
       "\n",
       "Lightning AI is a team within Facebook AI that focuses on developing open-source tools and technologies to support \n",
       "the development of machine learning models. PyTorch Lightning is one of their creations, and it is designed to \n",
       "simplify the process of building and deploying machine learning models.\n",
       "\n",
       "With PyTorch Lightning, you can build, train, and deploy machine learning models more efficiently. It provides a \n",
       "unified API for building and training models, as well as a unified API for deploying models. This makes it easier \n",
       "to switch between different models and frameworks, and it also makes it easier to share code between different \n",
       "teams.\n",
       "\n",
       "PyTorch Lightning is built on top of PyTorch, which means that it can be used with any PyTorch model. It also \n",
       "supports a wide range of data types, including images, text, and audio.\n",
       "\n",
       "Overall, PyTorch Lightning is a powerful tool for building and deploying machine learning models. It is \n",
       "open-source, which means that it is free to use and available to anyone.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "PyTorch Lightning is an open-source library developed by the Lightning AI team at Facebook. It is built on top of \n",
       "PyTorch, a popular deep learning framework, and is designed to make it easier to build, train, and deploy machine \n",
       "learning models.\n",
       "\n",
       "Lightning AI is a team within Facebook AI that focuses on developing open-source tools and technologies to support \n",
       "the development of machine learning models. PyTorch Lightning is one of their creations, and it is designed to \n",
       "simplify the process of building and deploying machine learning models.\n",
       "\n",
       "With PyTorch Lightning, you can build, train, and deploy machine learning models more efficiently. It provides a \n",
       "unified API for building and training models, as well as a unified API for deploying models. This makes it easier \n",
       "to switch between different models and frameworks, and it also makes it easier to share code between different \n",
       "teams.\n",
       "\n",
       "PyTorch Lightning is built on top of PyTorch, which means that it can be used with any PyTorch model. It also \n",
       "supports a wide range of data types, including images, text, and audio.\n",
       "\n",
       "Overall, PyTorch Lightning is a powerful tool for building and deploying machine learning models. It is \n",
       "open-source, which means that it is free to use and available to anyone.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = conversation(\n",
    "    \"PyTorch Lightning is an open-source library developed by Lightning AI team.\"\n",
    ")[\"response\"]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time for inference: 3.12 sec total, 6.08 tokens/sec\n",
      "Memory used: 13.41 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The team that developed PyTorch Lightning is the Lightning AI team at Facebook.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "The team that developed PyTorch Lightning is the Lightning AI team at Facebook.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = conversation(\n",
    "    \"who developed PyTorch Lightning? just give me the name of the team or person and nothing else.\"\n",
    ")[\"response\"]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://twitter.com/yanndubs/status/1681644889145237504?s=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
