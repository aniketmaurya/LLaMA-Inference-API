{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "longchat_template = \"\"\"A chat between a curious user and an artificial intelligence assistant.\n",
    "The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
    "USER: {input}\n",
    "ASSISTANT:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">A chat between a curious user and an artificial intelligence assistant.\n",
       "The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
       "USER: My name is Aniket\n",
       "ASSISTANT:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "A chat between a curious user and an artificial intelligence assistant.\n",
       "The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
       "USER: My name is Aniket\n",
       "ASSISTANT:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(longchat_template.format(input=\"My name is Aniket\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_inference import LLMInference, prepare_weights\n",
    "from rich import print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = str(prepare_weights(\"meta-llama/Llama-2-7b-chat-hf\"))\n",
    "# model = LLMInference(checkpoint_dir=path, quantize=\"bnb.nf4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights already exists at checkpoints/lmsys/longchat-7b-16k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Loading model 'checkpoints/lmsys/longchat-7b-16k/lit_model.pth' with {'org': 'lmsys', 'name': 'longchat-7b-16k', 'block_size': 2048, 'vocab_size': 32000, 'padding_multiple': 64, 'padded_vocab_size': 32000, 'n_layer': 32, 'n_head': 32, 'n_embd': 4096, 'rotary_percentage': 1.0, 'parallel_residual': False, 'bias': False, 'n_query_groups': 32, 'shared_attention_norm': False, '_norm_class': 'RMSNorm', 'norm_eps': 1e-06, '_mlp_class': 'LLaMAMLP', 'intermediate_size': 11008, 'condense_ratio': 8}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin /home/aniket/miniconda3/envs/parrot/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda121.so\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time to instantiate model: 9.08 seconds.\n",
      "Time to load the model weights: 12.65 seconds.\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    }
   ],
   "source": [
    "path = str(prepare_weights(\"lmsys/longchat-7b-16k\"))\n",
    "model = LLMInference(checkpoint_dir=path, quantize=\"bnb.nf4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "longchat_template = \"\"\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
    "USER: {input}\n",
    "ASSISTANT:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time for inference: 3.24 sec total, 3.70 tokens/sec\n",
      "Memory used: 5.61 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Hello Aniket! How can I assist you today?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Hello Aniket! How can I assist you today?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = model.chat(longchat_template.format(input=\"My name is Aniket\"))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time for inference: 7.62 sec total, 34.27 tokens/sec\n",
      "Memory used: 5.61 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Lightning strikes with power and might,\n",
       "A force of nature that doth ignite,\n",
       "A flash of light that pierces through the night,\n",
       "A moment of brilliance that doth light the fight.\n",
       "\n",
       "In the realm of AI, a new dawn breaks,\n",
       "A new age of intelligence that doth awakes,\n",
       "A new way of thinking, a new way of being,\n",
       "A new way of living, a new way of seeing.\n",
       "\n",
       "Lightning AI doth bring a new way to see,\n",
       "A new way to think, a new way to be,\n",
       "A new way to learn, a new way to grow,\n",
       "A new way to connect, a new way to know.\n",
       "\n",
       "With speed and precision, it doth compute,\n",
       "A new way to solve, a new way to compute,\n",
       "A new way to create, a new way to innovate,\n",
       "A new way to inspire, a new way to ignite.\n",
       "\n",
       "So let us welcome Lightning AI with open arms,\n",
       "A new way to live, a new way to embrace,\n",
       "A new way to learn, a new way to explore,\n",
       "A new way to connect, a new way to explore.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Lightning strikes with power and might,\n",
       "A force of nature that doth ignite,\n",
       "A flash of light that pierces through the night,\n",
       "A moment of brilliance that doth light the fight.\n",
       "\n",
       "In the realm of AI, a new dawn breaks,\n",
       "A new age of intelligence that doth awakes,\n",
       "A new way of thinking, a new way of being,\n",
       "A new way of living, a new way of seeing.\n",
       "\n",
       "Lightning AI doth bring a new way to see,\n",
       "A new way to think, a new way to be,\n",
       "A new way to learn, a new way to grow,\n",
       "A new way to connect, a new way to know.\n",
       "\n",
       "With speed and precision, it doth compute,\n",
       "A new way to solve, a new way to compute,\n",
       "A new way to create, a new way to innovate,\n",
       "A new way to inspire, a new way to ignite.\n",
       "\n",
       "So let us welcome Lightning AI with open arms,\n",
       "A new way to live, a new way to embrace,\n",
       "A new way to learn, a new way to explore,\n",
       "A new way to connect, a new way to explore.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = model.chat(longchat_template.format(input=\"Write a poem on Lightning AI\"))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time for inference: 0.45 sec total, 26.96 tokens/sec\n",
      "Memory used: 5.61 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Hello Aniket! How can I assist you today?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Hello Aniket! How can I assist you today?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time for inference: 1.09 sec total, 31.31 tokens/sec\n",
      "Memory used: 5.61 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">I'm sorry, but I don't have access to your name. Can you please provide me with more information or context about \n",
       "what you're looking for?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "I'm sorry, but I don't have access to your name. Can you please provide me with more information or context about \n",
       "what you're looking for?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = model.chat(longchat_template.format(input=\"My name is Aniket?\"))\n",
    "print(output)\n",
    "\n",
    "\n",
    "output = model.chat(longchat_template.format(input=\"What is my name?\"))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "longchat_template = \"\"\"A chat between a curious user and an artificial intelligence assistant.\n",
    "The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
    "Context:\n",
    "User: My name is Aniket\n",
    "Assistant: Hi, Aniket how are you?\n",
    "\n",
    "USER: {input}\n",
    "ASSISTANT:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time for inference: 0.30 sec total, 23.23 tokens/sec\n",
      "Memory used: 5.61 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Your name is Aniket.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Your name is Aniket.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "longchat_template = \"\"\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
    "Context:\n",
    "USER: My name is Aniket!\n",
    "ASSISTANT: How can I help you Aniket?\n",
    "USER: {input}\n",
    "ASSISTANT:\"\"\"\n",
    "\n",
    "output = model.chat(longchat_template.format(input=\"What is my name?\"))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time for inference: 0.66 sec total, 28.65 tokens/sec\n",
      "Memory used: 5.61 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Hello Aniket! Your name is Aniket. How can I assist you today?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Hello Aniket! Your name is Aniket. How can I assist you today?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "longchat_template = \"\"\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
    "Context:\n",
    "{history}\n",
    "USER: {input}\n",
    "ASSISTANT:\"\"\"\n",
    "\n",
    "history =\"USER: Hi, I am Aniket!\\nAssistant: How can I help you Aniket?\"\n",
    "\n",
    "\n",
    "output = model.chat(longchat_template.format(input=\"What is my name?\", history=history))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[PromptTemplate doc](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "longchat_template = \"\"\"A chat between a curious user and an artificial intelligence assistant.\n",
    "The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
    "Context:\n",
    "{history}\n",
    "USER: {input}\n",
    "ASSISTANT:\"\"\"\n",
    "\n",
    "longchat_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"input\", \"history\"], template=longchat_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">A chat between a curious user and an artificial intelligence assistant.\n",
       "The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
       "Context:\n",
       "USER: Hi, I am Aniket!\n",
       "Assistant: How can I help you Aniket?\n",
       "USER: What is my name?\n",
       "ASSISTANT:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "A chat between a curious user and an artificial intelligence assistant.\n",
       "The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
       "Context:\n",
       "USER: Hi, I am Aniket!\n",
       "Assistant: How can I help you Aniket?\n",
       "USER: What is my name?\n",
       "ASSISTANT:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(longchat_prompt_template.format(\n",
    "    input = \"What is my name?\",\n",
    "    history =\"USER: Hi, I am Aniket!\\nAssistant: How can I help you Aniket?\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "from llm_chain import LitGPTLLM\n",
    "\n",
    "\n",
    "llm = LitGPTLLM(model=model)\n",
    "\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    prompt=longchat_prompt_template,\n",
    "    verbose=False,\n",
    "    memory=ConversationBufferWindowMemory(ai_prefix=\"Assistant\", human_prefix=\"User\", k=2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time for inference: 0.45 sec total, 26.55 tokens/sec\n",
      "Memory used: 5.61 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello Aniket! How can I assist you today?'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation(\"hi, I am Aniket\")[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time for inference: 0.30 sec total, 23.19 tokens/sec\n",
      "Memory used: 5.61 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Aniket.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation(\"What is my name?\")[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time for inference: 0.51 sec total, 27.34 tokens/sec\n",
      "Memory used: 5.61 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'London is in the GMT (Greenwich Mean Time) timezone.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation(\"What is the timezone of London?\")[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatMessageHistory</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">messages</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'hi, I am Aniket'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>, <span style=\"color: #808000; text-decoration-color: #808000\">example</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Hello Aniket! How can I assist you today?'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>, <span style=\"color: #808000; text-decoration-color: #808000\">example</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'What is my name?'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>, <span style=\"color: #808000; text-decoration-color: #808000\">example</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Your name is Aniket.'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>, <span style=\"color: #808000; text-decoration-color: #808000\">example</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'What is the timezone of London?'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>, <span style=\"color: #808000; text-decoration-color: #808000\">example</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'London is in the GMT (Greenwich Mean Time) timezone.'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">example</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'What is the timezone of London?'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>, <span style=\"color: #808000; text-decoration-color: #808000\">example</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'London is in the GMT (Greenwich Mean Time) timezone.'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">example</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatMessageHistory\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mmessages\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mHumanMessage\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcontent\u001b[0m=\u001b[32m'hi, I am Aniket'\u001b[0m, \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m, \u001b[33mexample\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcontent\u001b[0m=\u001b[32m'Hello Aniket! How can I assist you today?'\u001b[0m, \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m, \u001b[33mexample\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mHumanMessage\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcontent\u001b[0m=\u001b[32m'What is my name?'\u001b[0m, \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m, \u001b[33mexample\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcontent\u001b[0m=\u001b[32m'Your name is Aniket.'\u001b[0m, \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m, \u001b[33mexample\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mHumanMessage\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcontent\u001b[0m=\u001b[32m'What is the timezone of London?'\u001b[0m, \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m, \u001b[33mexample\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mcontent\u001b[0m=\u001b[32m'London is in the GMT \u001b[0m\u001b[32m(\u001b[0m\u001b[32mGreenwich Mean Time\u001b[0m\u001b[32m)\u001b[0m\u001b[32m timezone.'\u001b[0m,\n",
       "            \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mexample\u001b[0m=\u001b[3;91mFalse\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mHumanMessage\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcontent\u001b[0m=\u001b[32m'What is the timezone of London?'\u001b[0m, \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m, \u001b[33mexample\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mcontent\u001b[0m=\u001b[32m'London is in the GMT \u001b[0m\u001b[32m(\u001b[0m\u001b[32mGreenwich Mean Time\u001b[0m\u001b[32m)\u001b[0m\u001b[32m timezone.'\u001b[0m,\n",
       "            \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mexample\u001b[0m=\u001b[3;91mFalse\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(conversation.memory.chat_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    prompt=longchat_prompt_template,\n",
    "    verbose=False,\n",
    "    memory=ConversationBufferMemory(ai_prefix=\"Assistant\", human_prefix=\"User\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = conversation(\n",
    "    \"PyTorch Lightning is an open-source library developed by Lightning AI team.\"\n",
    ")[\"response\"]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = conversation(\n",
    "    \"who developed PyTorch Lightning? just give me the name of the team or person and nothing else.\"\n",
    ")[\"response\"]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://twitter.com/yanndubs/status/1681644889145237504?s=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
